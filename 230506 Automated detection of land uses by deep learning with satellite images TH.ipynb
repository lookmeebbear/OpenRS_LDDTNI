{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1GdXkRyta2DU0uh5TwsqYw8bUF4gHHoEn","timestamp":1683187901882},{"file_id":"1mhXJzxm6EM6YVdwbJWw5KcZgHP4zVdsE","timestamp":1683163239985}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# Lab Session: การประยุกต์ใช้ deep learning กับภาพถ่ายดาวเทียมเพื่อตรวจจับการใช้ที่ดินโดยอัตโนมัติ\n","\n","<!-- Semantic Segmentation ของภาพถ่ายดาวเทียมสำหรับการทำแผนที่การใช้ที่ดิน-->\n","\n","ในส่วนของ Lab นี้ เราจะสำรวจว่า semantic segmentation สามารถใช้เพื่อตรวจจับการเปลี่ยนแปลงการใช้ที่ดินได้อย่างไร โดยเฉพาะอย่างยิ่ง *สวนยาง (ในการระบุ)* ส่วนนี้จะมอบการฝึกจริงในการตรวจจับการเปลี่ยนแปลงอัตโนมัติในการใช้ประโยชน์ที่ดินโดยใช้เครื่องมือฟรีซอฟต์แวร์ เช่น Colab และ QGIS และท่านจะได้เรียนรู้วิธีปรับแต่งโมเดลเพื่อขยายความครอบคลุมไปยังภูมิภาคอื่นๆ\n","\n","เนื้อหาของส่วนนี้รวมถึง:\n","1. บทนำสู่ lab session\n","2. การตรวจจับการเปลี่ยนแปลงโดยใช้ semantic segmentation\n","   * 2-1 การเตรียมข้อมูล (Data Preprocessing)\n","   * 2-2 Model training\n","   * 2-3 การประเมินโมเดลและการประเมินประสิทธิภาพ (Model evaluation and performance assessment)\n","   * 2-4 การนำโมเดลที่ผ่านการเทรนไปใช้ในพื้นที่เดิมในปีอื่น\n","   * 2-5 การตรวจจับตำแหน่งที่มีการเปลี่ยนแปลงครั้งใหญ่ที่สุด\n","3. การขยายความครอบคลุมของโมเดล\n","   * 3-1 ทดสอบโมเดลในภูมิภาคอื่น\n","   * 3-2 การเทรนโมเดลสำหรับภูมิภาคที่ใหญ่ขึ้นด้วยชุดข้อมูลใหม่\n","   * 3-3 การทดสอบโมเดลด้วยชุดข้อมูลใหม่\n"],"metadata":{"id":"_XQVVPQVoz7D"}},{"cell_type":"markdown","source":["# ส่วนที่ 0. การติดตั้งเบื้องต้น\n","\n","นี่คือโค้ดสำหรับการติดตั้งแพ็คเกจที่จำเป็นและชุดข้อมูลสาธิต"],"metadata":{"id":"HNdp0T4AZZRq"}},{"cell_type":"code","source":["# update apt repositories and install necessary packages\n","!apt update\n","!apt install gdal-bin imagemagick parallel bc jq\n","\n","# install required Python packages\n","!pip install segmentation-models tensorflow_addons geopandas rasterio scikit-learn matplotlib\n","\n","# Getting the custom codes for satellite data processing.\n","!rm -rf semantic_segmentation_training\n","!git clone https://github.com/GLODAL/semantic_segmentation_training.git\n","\n","# Getting the demo dataset\n","!rm -rf *.zip 2-1_img 2-1_patch 2-4_img 3-3_train_img 3_img ann vec\n","!wget https://glodal-inc.com/downloads/demo_rubber.zip\n","#!wget https://glodal-inc.com/downloads/demo_paddy.zip\n","#!wget https://glodal-inc.com/downloads/demo_pineapple.zip\n","!unzip -o *.zip\n"],"metadata":{"id":"J3Jd0vL8ZV6O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ส่วนที่ 1. บทนำเกี่ยวกับ Semantic Segmentation ของภาพถ่ายดาวเทียมสำหรับการทำแผนที่สวนยาง\n","\n","[Semantic Segmentation](https://paperswithcode.com/task/semantic-segmentation) คือเทคนิคหนึ่งของ Computer Vision ที่มีหน้าที่ในการแยกส่วนและจำแนกวัตถุแต่ละวัตถุในภาพแยกจากกันโดยกำหนดคลาสในแต่ละจุดพิคเซลว่าเป็นคลาสอะไร เป็นเครื่องมืออันทรงพลังสำหรับใช้วิเคราะห์ภาพถ่ายดาวเทียม เนื่องจากช่วยให้เราสามารถจัดทำแผนที่และติดตามการเปลี่ยนแปลงการใช้ที่ดินได้อย่างแม่นยำเมื่อเวลาผ่านไป ด้วยการใช้ Semantic Segmentation เพื่อตรวจหาการเปลี่ยนแปลงการใช้ที่ดิน ทำให้เราเข้าใจไดนามิกของการใช้ที่ดินได้ดีขึ้น\n","\n","ใน lab session นี้ เราจะใช้ภาพจาก Planet และ/หรือ Landsat, Google Colab และ QGIS เพื่อทำการแบ่งส่วน(segmentation)และการแสดงภาพ(visualization)\n","\n","<dl>Planet</dl>\n","<dd>ชุดข้อมูลภาพถ่ายจาก Planet ที่ใช้ใน lab session นี้ประกอบด้วยภาพสีจริง(true color composite)ที่มีความละเอียดเชิงพื้นที่ 5 เมตร ด้านล่างเป็นตัวอย่างสำหรับนาข้าว ชุดข้อมูลนี้จัดทำขึ้นภายใต้โครงการ NICFI ซึ่งได้รับการสนับสนุนจากรัฐบาลนอร์เวย์ สำหรับข้อมูลเพิ่มเติมเกี่ยวกับชุดข้อมูล โปรดไปที่ <a href=\"https://www.planet.com/nicfi/\">here</a>.<br><img src=\"https://drive.google.com/uc?export=download&id=1mfWvzqxRQKo12jBHSkyRrwBJ_-OYl2V3\" width=320>\n","</dd>\n","\n","<dl>Landsat</dl>\n","<dd>ชุดข้อมูลภาพถ่ายจาก Landsat นิยมใช้ทำแผนที่การใช้ที่ดิน เทคนิคหนึ่งสำหรับสิ่งนี้คือการใช้การผสมสี false color composite เช่น band 564 (รูปภาพด้านล่าง) ซึ่งสามารถช่วยแยกความแตกต่างระหว่างพืชประเภทต่างๆ ด้วยความละเอียดเชิงพื้นที่ 30 เมตร ภาพถ่าย Landsat สามารถเก็บรายละเอียดของสวนยางได้ สำหรับข้อมูลเพิ่มเติมเกี่ยวกับภาพถ่าย Landsat โปรดไปที่ <a href=\"https://www.usgs.gov/landsat-missions\">this website.</a><br><img src=\"https://drive.google.com/uc?export=download&id=1nMq55ahWwTD7Cd-DE0-hgqiKLSjaT6jh\" width=320>\n","</dd>\n","\n","<dl>ข้อมูลภาคสนาม Ground truth (GT) dataset</dl>\n","<dd>ข้อมูลชั้นเวกเตอร์สำหรับสวนยางพาราในรูปแบบโพลีกอนได้รับจากกรมพัฒนาที่ดิน (LDD) สำหรับปีที่เกี่ยวข้อง ชั้นข้อมูลนี้ใช้สำหรับสร้างป้ายกำกับภาพ(labeled images)ของการ training และตรวจสอบความถูกต้องของโมเดล ตลอดจนการประเมินความแม่นยำของผลลัพธ์ที่คาดการณ์ไว้(predicted results) นี่คือผลลัพธ์ที่คาดการณ์ไว้ </dd>\n","\n","<dl>Google Colab</dl>\n","<dd>Google Colab เป็นแพลตฟอร์มคลาวด์ที่ไม่มีค่าใช้จ่ายซึ่งเปิดโอกาสให้ผู้ใช้เขียนและรันโค้ด Python ใน Jupyter Notebook environment พร้อมคุณสมบัติดังต่อไปนี้: <br>1) พร้อมสำหรับการเรียนรู้และฝึกฝน machine learning, <br>2) เข้าถึง GPUs ฟรี, <br>3) การผสานรวมกับ Google Drive และการแชร์ที่ง่ายดาย <br>\n","หากต้องการค้นหาข้อมูลเพิ่มเติมเกี่ยวกับ Google Colab โปรดดู <a href=\"https://www.youtube.com/watch?v=inN8seMm7UI\">this video</a></dd>\n","\n","\n","\n","<dl>QGIS</dl>\n","<dd>QGIS เป็นซอฟต์แวร์ GIS ที่ฟรีและเป็น open-source ช่วยให้ผู้ใช้สามารถวิเคราะห์ ดู และแก้ไขข้อมูลเชิงพื้นที่ได้ สำหรับข้อมูลเพิ่มเติมเกี่ยวกับ QGIS หากต้องการดาวน์โหลดและติดตั้ง โปรดไปที่เว็บไซต์ <a href=\"https://www.qgis.org/en/site/\">here.</a></dd>\n","\n","<dl><a href=\"https://github.com/qubvel/segmentation_models\">Segmentation Models</a></dl>\n","<dd>ในงานด้านคอมพิวเตอร์ semantic segmentation models ใช้ deep learning algorithms เพื่อระบุและแบ่งส่วนวัตถุภายในภาพในระดับพิกเซล เช่น สวนยางพารา Semantic segmentation models ประเภทต่างๆ เช่น fully convolutional networks (FCN), U-Net, และ Mask R-CNN ต่างมีจุดแข็งและจุดอ่อนของตัวเอง ใน lab session นี้ semantic segmentation model <a href=\"https://paperswithcode.com/method/fpn\">Feature Pynamic Network (FPN)</a> สามารถใช้เพื่อระบุและจัดทำแผนที่พื้นที่สวนยางภายในภาพ Landsat ด้วยการ training model บน labeled data ซึ่งสามารถสร้างขึ้นโดยใช้ข้อมูลภาคสนาม ทำให้สามารถเรียนรู้ที่จะรับรู้ลักษณะเฉพาะของสวนยางและแยกความแตกต่างจากพืชพันธุ์หรือสิ่งปกคลุมดินชนิดอื่นๆได้</dd>"],"metadata":{"id":"-jcHlILopwm_"}},{"cell_type":"markdown","source":["# ส่วนที่ 2. การตรวจจับการเปลี่ยนแปลงโดยใช้ semantic segmentation\n","ในส่วนนี้ เราจะครอบคลุมขั้นตอนที่เกี่ยวข้องในการเตรียมข้อมูลสำหรับการตรวจจับการเปลี่ยนแปลง, การเทรนโมเดล(training the model) และการประเมินประสิทธิภาพของโมเดล(model's performance)"],"metadata":{"id":"6wWcFIHX-rF_"}},{"cell_type":"markdown","source":["## 2-1 การเตรียมข้อมูล (Data Pre-processing)\n","ในส่วนย่อยนี้ เราจะเตรียมภาพถ่ายดาวเทียมและข้อมูลภาคสนาม(GT) สำหรับ semantic segmentation ต่อไปคือวัตถุประสงค์จากส่วนนี้:\n","1.  ทำความเข้าใจกับรูปแบบ(formats)ของชุดข้อมูล training ซึ่งรวมถึง\n","  *   Patch images\n","  *   ช่อง RGB และป้ายกำกับ (RGB chanels and labels)\n","  *   ตัวอย่างโครงสร้างไฟล์ที่ใช้สำหรับ lab session นี้\n","\n","2.   ทำความเข้าใจขั้นตอนก่อนการประมวลผลที่จำเป็นในการเตรียมภาพถ่ายดาวเทียมและข้อมูล GT สำหรับ semantic segmentation \n"],"metadata":{"id":"iwwYx496_0Z7"}},{"cell_type":"markdown","source":["### 2.1.1 การสร้าง patch images สำหรับ model training\n","\n","ข้อมูลรูปภาพต้องจัดรูปแบบเป็น patche images มีขนาด 64x64, 128x128, หรือ 224x224, ... ตามสมมติฐานอัลกอริธึมของ CNN นอกจากนี้ ข้อมูล GT ของการใช้ที่ดินจัดทำเป็นเลเยอร์เวกเตอร์โพลีกอน ซึ่งไม่เหมาะสำหรับ CNN ดังนั้นจึงต้องใช้กระบวนการที่ยุ่งยากเล็กน้อยเพื่อใช้ใน CNN\n","\n","เนื่องจากโมเดล semantic segmentation ต้องการข้อมูลอินพุตในรูปแบบของภาพราสเตอร์, เราจำเป็นต้องแปลงข้อมูลเวกเตอร์เป็นรูปแบบภาพราสเตอร์ ภาพราสเตอร์จะมีขนาดและความละเอียดเดียวกันกับภาพถ่ายดาวเทียมอินพุต โดยแต่ละพิกเซลจะกำหนดค่าเป็น 1 หรือ 0 ขึ้นอยู่กับว่าภาพนั้นอยู่ในบริเวณที่สนใจหรือไม่ โค้ดต่อไปนี้สาธิตวิธีการแปลงเป็นไฟล์ราสเตอร์\n","\n","ในภาคปฏิบัตินี้ เราได้กำหนด patch size เป็น 128x128 ซึ่งเป็นขนาดที่เพียงพอในการจับภาพลักษณะการใช้ที่ดิน โดยทั่วไปแล้ว ขนาดใหญ่กว่าย่อมดีกว่า แต่ patch size จะถูกจำกัดโดยทรัพยากรในการคำนวณ เช่น RAM หรือหน่วยความจำ"],"metadata":{"id":"IkPpgKoZZ9mM"}},{"cell_type":"code","source":["# Configure parameters for patch images.\n","n_patch = 50     # number of patches per image file. For this practice n_patch ~ 200 will be preferred.\n","                 # Since the patch generation processes five types of data augumentation (vertical flip, horizental flip, and rotation 90, 180, and 270 degrees) for every patches, actual number of patches are six times of this paramter.\n","patch_size = 128 # size of patch images.\n","\n","in_vec_dir = 'vec/'\n","in_img_dir = '2-1_img/'\n","out_ann_dir = 'ann/'\n","out_patch_dir = '2-1_patch'\n","\n","import os\n","# The module is prepared in shell script. You can see detail by browsing the file inside or https://github.com/GLODAL/semantic_segmentation_training/blob/main/rasterize_polygon_layers.sh\n","os.system('bash semantic_segmentation_training/rasterize_polygon_layers.sh \"'\n","          + in_vec_dir + '\" \"'\n","          + in_img_dir + '\" \"'\n","          + out_ann_dir +'\"'\n","          )\n"],"metadata":{"id":"AdRklnlejNw9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ต่อไป เราจะสร้างชุดของ patches จากภาพราสเตอร์และคำอธิบายประกอบที่เกี่ยวข้อง"],"metadata":{"id":"AsephNMrdQrR"}},{"cell_type":"code","source":["import os\n","in_ann_dir = out_ann_dir\n","# The module is prepared in shell script. You can see detail by browsing the file inside or https://github.com/GLODAL/semantic_segmentation_training/blob/main/gen_training_patches.sh\n","os.system('bash semantic_segmentation_training/gen_training_patches.sh \"' \n","         + in_img_dir + '\" \"' \n","          + in_ann_dir + '\" ' \n","          + str(patch_size) + ' '\n","          + str(n_patch) + ' '\n","          + out_patch_dir\n","          )\n"],"metadata":{"id":"ENK9oA5KdPU3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["หลังจากสร้างข้อมูล patch แล้ว ท่านจะพบไฟล์ patch image ในโฟลเดอร์ ใต้ '2-1_patch' พร้อมด้วยโฟลเดอร์ย่อย 'patch_img' และ 'patch_ann' สำหรับภาพดาวเทียมและข้อมูล label จากชั้นการใช้ประโยชน์ที่ดิน ตามลำดับ\n","\n","นี่คือตัวอย่างของ patch ของรูปภาพและคำอธิบายประกอบที่เกี่ยวข้องซึ่งสร้างขึ้นโดยใช้โค้ดด้านบน\n","<br><img src=\"https://drive.google.com/uc?export=download&id=1nUWSRHlnTqbtXllkNuawClpSQLSrbqJM\" width=640>\n","\n","หากต้องการเรียกดูผลลัพธ์ ให้ทำตามขั้นตอนด้านล่าง:\n","1. คลิกไอคอน \"Files\" ที่แผงด้านขวา \n","2. หลายโฟลเดอร์จะปรากฏ:\n","  - 2-1_image: ภาพ Landsat สำหรับชุดข้อมูล training ของปีฐาน\n","  - 2-1_patch: Patch image และคำอธิบายประกอบที่สร้างใน 2_1\n","  - 2-4_image: ภาพ Landsat สำหรับชุดข้อมูล testing ของปีที่ต้องการทำนาย\n","  - 3-3_train_img: ภาพ Landsat สำหรับชุดข้อมูล training ของสองภูมิภาคสำหรับปีฐาน\n","  - 3_img: ภาพ Landsat สำหรับชุดข้อมูล testing ของปีที่ต้องการทำนาย\n","  - ann: ชั้นการใช้ประโยชน์ที่ดินราสเตอร์ใน 2-1\n","  - semantic_segmentation_training: รหัสแบบกำหนดเองสำหรับการประมวลผลข้อมูลดาวเทียมที่ดาวน์โหลดในส่วนที่ 0 \n","  - vec: Polygon layer ของคลาสย่อยจากชั้นการใช้ที่ดินที่จัดทำโดย LDD\n","3. เลือกโฟลเดอร์ชื่อ **2-1_patch** มีโฟลเดอร์ย่อยสองโฟลเดอร์ภายในโฟลเดอร์นี้ชื่อ **patch_ann** และ **patch_image** สิ่งเหล่านี้บ่งบอกถึง patch ของ label data และภาพถ่ายดาวเทียม\n","4. ภายในโฟลเดอร์ย่อย **patch_ann** หรือ **patch_image** สำหรับ หนึ่ง patch image จะมีไฟล์ TIFF หกไฟล์ที่แสดงแทนไฟล์ TIFF ดั้งเดิม การหมุน 90, 180 และ 270 องศา พลิก และปัด สิ่งเหล่านี้ถูกสร้างขึ้นโดยใช้เทคนิคการเสริมข้อมูลเพื่อเพิ่มจำนวน training data\n","5. คลิกขวาที่รูปภาพ TIFF ใดก็ได้และดาวน์โหลด **โปรดแน่ใจว่าคุณดาวน์โหลดภาพ TIFF ด้วยชื่อเดียวกันกับไฟล์คำอธิบายประกอบที่เกี่ยวข้อง**\n","6. นำเข้ารูปภาพที่ดาวน์โหลดมาใน QGIS เพื่อให้แสดงภาพ\n","\n","เพื่ออธิบายขั้นตอนข้างต้น โปรดดูรูปด้านล่าง\n","<br><img src=\"https://drive.google.com/uc?export=download&id=1nW-p4kP2fGC5Dd4qTDdMnM4VPUbjDdXh\" width=640>\n","\n"],"metadata":{"id":"OJdifG7OeFxN"}},{"cell_type":"markdown","source":["## 2.2 Model training\n","ในส่วนย่อยนี้ เราจะมุ่งเน้นไปที่การเทรนนิ่ง semantic segmentation โดยใช้ข้อมูลที่เตรียมไว้จากหัวข้อ นี่คือประเด็นหลักจากส่วนนี้:\n","* ทำความเข้าใจส่วนประกอบของ model training สำหรับ semntic segmentation\n","* ทำความเข้าใจการจัดการ model training เพื่อการใช้งานจริง\n","* ทำความเข้าใจกับกรณีความล้มเหลวทั่วไประหว่าง model training และตัวเลือกที่มีอยู่สำหรับการปรับปรุงกระบวนการ model training\n","\n","ในเซสชันแล็บนี้ เราใช้ [FPN](https://paperswithcode.com/method/fpn) model เป็นเครือข่ายประสาทเทียมแบบเต็มรูปแบบ (convolutional neural network) ที่ใช้สำหรับ binary segmentation เช่น การจำแนกพิกเซลในส่วนหน้าและพื้นหลัง ท่านสามารถดูรายละเอียดทางทฤษฎีได้จากลิงค์ด้านบน FPN มีประสิทธิภาพที่ได้เปรียบในแอปพลิเคชันสำหรับการสำรวจระยะไกลผ่านดาวเทียมมากกว่า UNet โมเดลที่นิยมมากที่สุดใน semantic segmentatoin"],"metadata":{"id":"JQjQPN0ejZNP"}},{"cell_type":"markdown","source":["### 2.2.1 การสร้าง image generator สำหรับกระบวนการ batch \n","วัตถุประสงค์ของส่วนย่อยนี้คือการกำหนดฟังก์ชันสำหรับการสร้าง batch ซึ่งป้อน patch images ไปยัง model training ท่านสามารถดูขั้นตอนการ\n","การสร้าง batched ของภาพถ่ายดาวเทียมและ label image ที่เกี่ยวข้องสำหรับ training และ testing ในฟังก์ชั่น image_generator\n","\n","FYI: Keras จัดเตรียมฟังก์ชัน [ImageDataGenerator](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) เพื่อจุดประสงค์เดียวกัน แต่ฟังก์ชันนี้ไม่เหมาะสำหรับ semantic segmentation โดยใช้ชุดข้อมูลของพวกเรา"],"metadata":{"id":"HdCG-d0SkHMW"}},{"cell_type":"code","source":["validation_split = 0.2 # Float. Fraction of images reserved for validation (strictly between 0 and 1).\n","batch_size = 32\n","\n","import numpy as np, random, glob\n","from PIL import Image\n","\n","# generating batches of images and their corresponding masks\n","def image_generator(files, batch_size = batch_size, sz = (patch_size, patch_size)):\n","  while True: \n","    \n","    #extract a random batch \n","    batch = np.random.choice(files, size = batch_size)    \n","    \n","    #variables for collecting batches of inputs and outputs \n","    batch_x = []\n","    batch_y = []\n","    \n","    for f in batch:\n","        #get the masks. Note that masks are png files \n","        try:\n","          mask = Image.open(f.replace(\"patch_img\",\"patch_ann\"))\n","        except:\n","          continue\n","        mask = np.array(mask.resize(sz))\n","        batch_y.append(mask)\n","\n","        #preprocess the raw images \n","        raw = Image.open(f)\n","        raw = raw.resize(sz)\n","        raw = np.array(raw)\n","\n","        #check the number of channels because some of the images are RGBA or GRAY\n","        if len(raw.shape) == 2:\n","          raw = np.stack((raw,)*3, axis=-1)\n","        else:\n","          raw = raw[:,:,0:3]\n","        \n","        batch_x.append(raw)\n","\n","    #preprocess a batch of images and masks \n","    batch_x = np.array(batch_x)/255.\n","    batch_y = np.array(batch_y)\n","    batch_y = np.expand_dims(batch_y,axis = 3)\n","\n","    yield (batch_x, batch_y.astype(np.float32))\n","\n","# list of all files in the directory specified by patch_img\n","all_files = glob.glob(out_patch_dir + '/patch_img/*.tif')\n","\n","# randomly shuffles the order of the files in all_files\n","random.shuffle(all_files) \n","\n","#split into training and testing\n","split = int((1-validation_split) * len(all_files))\n","train_files = all_files[0:split]\n","test_files  = all_files[split:]\n","\n","train_generator = image_generator(train_files, batch_size = batch_size) # creates a generator for the training set \n","test_generator  = image_generator(test_files, batch_size = batch_size) # creates a generator for the testing set \n","\n","# retrieves the next batch of images and masks from the training set generator\n","x, y= next(train_generator)\n"],"metadata":{"id":"To8qDnYAID7V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["โค้ดด้านล่างจะสร้างพล็อตรูปภาพที่แสดงรูปภาพอินพุตและคำอธิบายประกอบที่เกี่ยวข้อง เราอาจจะเปลี่ยน array index เพื่อดูตัวอย่างอื่นๆ"],"metadata":{"id":"NnKX4MFZmDmz"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# generates an image plot of an input image and its corresponding mask\n","plt.axis('off')\n","i = 1\n","img = x[i]\n","msk = y[i].squeeze()\n","msk = np.stack((msk,)*3, axis=-1)\n","print(img.shape)\n","\n","plt.imshow(np.concatenate([img, msk, img*msk], axis = 1))"],"metadata":{"id":"V0UbRh4SI9u3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.2.2 กำลังเตรียมโมเดลที่จะเทรน\n","\n","ส่วนนี้เป็นการเตรียมโมเดลโดยอ้างอิงโมเดลที่เทรนไว้ล่วงหน้าซึ่งจัดทำโดย [Segmentation Models](https://github.com/qubvel/segmentation_models) ส่วนนี้มีพารามิเตอร์หลักสำหรับ model training (หรือที่เรียกว่าไฮเปอร์พารามิเตอร์) ดังนั้นท่านอาจกลับมาที่ส่วนนี้เพื่อปรับแต่ง model training หากโมเดลล้มเหลว"],"metadata":{"id":"SdRssoe4mVA_"}},{"cell_type":"code","source":["backbone = 'efficientnetb1'\n","\n","os.environ['SM_FRAMEWORK'] = \"tf.keras\"\n","import segmentation_models as sm\n","import tensorflow_addons as tfa\n","\n","model = sm.FPN(backbone, classes=1, activation='sigmoid')\n","model.compile(\n","    optimizer = tfa.optimizers.RectifiedAdam(),\n","    loss = sm.losses.bce_jaccard_loss,\n","    metrics = ['accuracy',sm.metrics.iou_score]\n",")"],"metadata":{"id":"vMlxZbzqJ6QF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.2.3 การสร้างฟังก์ชั่น callback \n","\n","เรากำหนดฟังก์ชัน callback ซึ่งเรียกใช้ทุกจุดเริ่มต้นหรือจุดสิ้นสุดของ epoch เพื่อการจัดการกระบวนการ training ที่ดีขึ้น เรากำหนดสองฟังก์ชัน callback\n","\n","1. การเซฟโมเดลที่ epochs มีประสิทธิภาพสูงสุด สิ่งนี้มีประโยชน์สำหรับการเก็บโมเดลเพื่อใช้กับชุดข้อมูลอื่น ไม่เพียงแต่ใน model training เท่านั้น ดู [คู่มือผู้ใช้ Keras](https://keras.io/api/callbacks/model_checkpoint/) สำหรับรายละเอียดเพิ่มเติม\n","2. แสดงผล testing ทุก epoch TensorFlow แสดง performance metrics ในทุกช่วงท้ายของ epoch แต่บางครั้งก็ไม่มีประโยชน์สำหรับวิธีที่โมเดลแสดงผลลัพธ์ของการทำนาย สิ่งนี้ช่วยให้เข้าใจโดยสังเขปเกี่ยวกับความคืบหน้าใน model training\n","\n","\n"],"metadata":{"id":"-0AO__17pFLX"}},{"cell_type":"code","source":["checkpoint_file = '2-2_checkpoint.hdf5'\n","\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_file, save_weights_only=True, save_best_only=True)\n","\n","# inheritance for training process plot \n","class PlotLearning(tf.keras.callbacks.Callback):\n","\n","    def on_train_begin(self, logs={}):\n","        self.i = 0\n","        self.x = []\n","        self.losses = []\n","        self.val_losses = []\n","        self.acc = []\n","        self.val_acc = []\n","        #self.fig = plt.figure()\n","        self.logs = []\n","        \n","    def on_epoch_end(self, epoch, logs={}):\n","        \n","        #choose a random test image and preprocess\n","        path = np.random.choice(test_files)\n","        raw = Image.open(path)\n","        raw = np.array(raw)/255.\n","        #print(raw.shape)\n","        raw = raw[:,:,0:3]\n","        \n","        #predict the mask \n","        pred = model.predict(np.expand_dims(raw, 0))\n","        \n","        #mask post-processing \n","        msk  = pred.squeeze()\n","        msk = np.stack((msk,)*3, axis=-1)\n","        msk[msk >= 0.5] = 1 \n","        msk[msk < 0.5] = 0 \n","        \n","        #show the mask and the segmented image \n","        combined = np.concatenate([raw, msk, raw* msk], axis = 1)\n","        plt.axis('off')\n","        plt.imshow(combined)\n","        plt.show()"],"metadata":{"id":"fBIJmG0gJ_ON"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2-2-4 Training\n","ขั้นตอนสุดท้ายของส่วนนี้คือการปฏิบัติ model training โดยใช้ชุด training และประเมินประสิทธิภาพโดยใช้ชุด testing"],"metadata":{"id":"c1mvcJxapNLm"}},{"cell_type":"code","source":["n_epoch = 20\n","\n","import math, glob\n","train_step = math.ceil(len(glob.glob(out_patch_dir+'/patch_img/*.tif')) * (1 - validation_split) / batch_size)\n","test_step = math.ceil(len(glob.glob(out_patch_dir+'/patch_img/*.tif')) * validation_split / batch_size)\n","\n","model_history = model.fit(train_generator, \n","                          epochs = n_epoch, \n","                          steps_per_epoch = train_step,\n","                          validation_data = test_generator, \n","                          validation_steps = test_step,\n","                          callbacks = [checkpointer,PlotLearning()], \n","                          verbose = 1)\n"],"metadata":{"id":"EIA67iE6KHZn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["หลังจาก model training ลองพล็อต learning curves เพื่อประเมินความถูกต้องของ model training"],"metadata":{"id":"jKgBzByx-N_H"}},{"cell_type":"code","source":["# creating a plot of the training and validation loss over the number of epochs\n","# extract the training loss and validation loss values \n","loss = model_history.history['loss']\n","val_loss = model_history.history['val_loss']\n","\n","# creates a range object for the number of epochs\n","epochs = range(n_epoch)\n","\n","plt.figure()\n","plt.plot(epochs, loss, 'r', label='Training loss')\n","plt.plot(epochs, val_loss, 'bo', label='Validation loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss Value')\n","plt.ylim([np.min(np.hstack([loss, val_loss])), np.max(np.hstack([loss, val_loss]))])\n","plt.legend()\n","plt.show()"],"metadata":{"id":"4RJk0-2MKNm-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["หาก learning curve ล้มเหลวใน overfitting ซึ่ง validation loss เพิ่มขึ้นตามช่วงเวลาเช่นรูปด้านล่าง model จะทำงานได้ไม่ดีนักเนื่องจาก model ถูก localized มากเกินไปใน model training ในขณะที่ชุดข้อมูลอื่น ๆ เหมาะสมแค่เพียงเล็กน้อย\n","\n","เราขอแนะนำให้เปลี่ยนพารามิเตอร์ดังต่อไปนี้:\n","\n","1. เพิ่มจำนวน patches ใน <a href=\"https://colab.research.google.com/drive/1GdXkRyta2DU0uh5TwsqYw8bUF4gHHoEn?authuser=4#scrollTo=IkPpgKoZZ9mM&line=7&uniqifier=1\">Section 2-1</a>. รูปแบบภาพที่หลากหลายมากขึ้นใน training data มักจะช่วยให้ model รับรู้ถึงคุณลักษณะที่ละเอียดอ่อนของการใช้ที่ดินได้ อย่างไรก็ตาม อาจใช้เวลานานขึ้นใน model training ท่านควรพิจารณาการแลกเปลี่ยนระหว่างเวลาและประสิทธิภาพ\n","2. เปลี่ยน backbone ของ model. The backbone ตั้งอยู่ใน <a href=\"https://colab.research.google.com/drive/1GdXkRyta2DU0uh5TwsqYw8bUF4gHHoEn?authuser=4#scrollTo=SdRssoe4mVA_&line=3&uniqifier=1\">Section 2-2-2</a>, \"efficientnetb2\" มีชื่อเสียงในด้าน robustness จน overfitting อย่างไรก็ตาม ท่านสามารถแทนที่ด้วย backbone ที่ซับซ้อนน้อยกว่าเช่น \"efficiencynetb0\" และ \"resnet50\" ท่านสามารถค้นหาตัวเลือกใน [the website of Segmentation Models](https://github.com/qubvel/segmentation_models#models-and-backbones).\n","3. ลด batch size. โดยทั่วไป, [batch sizes ขนาดใหญ่อาจเป็นสาเหตุเกิด overfitting.](https://stats.stackexchange.com/questions/266368/deep-learning-why-does-increase-batch-size-cause-overfitting-and-how-does-one-r) คุณสามารถกำหนดค่า batch_size ได้ใน [Section 2-2-1](https://colab.research.google.com/drive/1GdXkRyta2DU0uh5TwsqYw8bUF4gHHoEn?authuser=4#scrollTo=HdCG-d0SkHMW). อย่างไรก็ตาม อาจใช้เวลานานขึ้นใน model training ท่านควรพิจารณาการแลกเปลี่ยนระหว่างเวลาและประสิทธิภาพ\n","\n","|Good model training|Failure in overfitting|\n","|-------------------|----------------------|\n","|<img src=\"https://machinelearningmastery.com/wp-content/uploads/2018/12/Example-of-Train-and-Validation-Learning-Curves-Showing-A-Good-Fit.png\" width=\"640\">|<img src=\"https://machinelearningmastery.com/wp-content/uploads/2018/12/Example-of-Train-and-Validation-Learning-Curves-Showing-An-Overfit-Model.png\" width=\"640\">|\n","\n","Source: <a href=\"https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/\">How to use Learning Curves to Diagnose Machine Learning Model Performance</a>\n"],"metadata":{"id":"b2cv89PG0hfA"}},{"cell_type":"markdown","source":["## 2-3 การประเมินโมเดล\n","จะได้เรียนรู้เกี่ยวกับ:\n","*  ทำความเข้าใจการประเมินโมเดลในทางปฏิบัติโดยใช้เมตริก เช่น IoU, Learning Curve และ overall accuracy และการตีความตัวบ่งชี้\n","\n","model training ใช้เฉพาะข้อมูลที่สุ่มตัวอย่างเท่านั้น เนื่องจากข้อมูลการสำรวจระยะไกลโดยทั่วไปมีขนาดใหญ่เกินกว่าจะโหลดข้อมูลทั้งหมดไปยังกระบวนการ เราต้องใช้โมเดลที่ผ่านการเทรนกับขอบเขตทั้งหมดของภาพถ่ายดาวเทียมเพื่อการประเมินความแม่นยำ\n","\n","เราฝึก testing โมเดลที่ผ่านการเทรนใน 2.2 โดยคำนวณ confusion matrix ระหว่างผลการทำนายและชั้นการใช้ประโยชน์ที่ดิน"],"metadata":{"id":"abNfVU0yD-Uj"}},{"cell_type":"markdown","source":["### 2-3-1 การนำโมเดลไปใช้กับชุดข้อมูลรูปภาพ\n","\n","ภาพถ่ายดาวเทียมจะต้องถูกแบ่งออกเป็น patches มีขนาดพิกเซล, multiples of patch images สำหรับชุดข้อมูล training เราแบ่งภาพถ่ายดาวเทียมออกเป็น patchesมีขนาด 256x256 สำหรับอินพุตไปยัง trained model.  "],"metadata":{"id":"hAgFoYQEFrj8"}},{"cell_type":"markdown","source":["ขั้นแรก เราตั้งค่า model trained in 2-2 น 2-2 เป็นเฟรมเวิร์ก Segmantation Models"],"metadata":{"id":"U24Ed9ENESGQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qugg5BS5bAv4"},"outputs":[],"source":["## Specify the model trained in Section 2-2 ##\n","model_file = '2-2_checkpoint.hdf5'\n","##############################################\n","\n","import os\n","os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n","import segmentation_models as sm\n","\n","# Set a model architecture and backbone same to the model for loading.\n","model = sm.FPN('efficientnetb2', classes=1, activation='sigmoid')\n","# Set a path to the trained model file for testing.\n","model.load_weights(model_file)"]},{"cell_type":"markdown","source":["จากนั้น ใช้โมเดลที่โหลดกับชุดข้อมูลรูปภาพจากภูมิภาคอื่น ไฟล์สคริปต์ *split_images_for_testing.sh* มีไว้สำหรับแยกภาพถ่ายดาวเทียม และ *merge_annotation_for_testing.sh* มีไว้สำหรับการรวม prediction outputs. ท่านสามารถดูรายละเอียดได้ที่ที่เก็บ GitHub"],"metadata":{"id":"0PtbNGFjETLd"}},{"cell_type":"code","source":["input_geotiff = '2-1_img/Prachuap A, Year 2018.tif'\n","output_pred_file = \"test_results/2-3-1 Prachuap A, Year 2018.tif.pred.tif\"\n","test_patch_size = 256\n","\n","import os, tempfile, shutil, glob\n","import numpy as np\n","from PIL import Image\n","os.environ['SM_FRAMEWORK'] = \"tf.keras\"\n","import segmentation_models as sm\n","\n","workdir = tempfile.mkdtemp()\n","os.system('bash semantic_segmentation_training/split_images_for_testing.sh \"'  # https://github.com/GLODAL/semantic_segmentation_training/blob/main/split_images_for_testing.sh \n","          + input_geotiff + '\" ' \n","          + workdir + ' ' \n","          + str(test_patch_size) )\n","\n","os.makedirs(workdir + '/pat_prd/', exist_ok=True)\n","for f in glob.glob(workdir + '/pat_img/*.tif'):\n","  raw = Image.open(f)\n","  out_patch_size = raw.size[0]\n","  raw = np.array(raw)\n","  raw = np.array(raw)/255.\n","  raw = raw[:,:,0:3]\n","\n","  pred = model.predict(np.expand_dims(raw, 0))\n","  pred[pred >= 0.5] = 1\n","  pred[pred < 0.5] = 0\n","  pred = pred.astype(np.uint8).reshape([out_patch_size,out_patch_size])\n","  im = Image.fromarray(pred)\n","  im.save(workdir + '/pat_prd/' + os.path.basename(f) + '.pred.tif')\n","\n","os.system('bash semantic_segmentation_training/merge_annotation_for_testing.sh ' # https://github.com/GLODAL/semantic_segmentation_training/blob/main/merge_annotation_for_testing.sh\n","          + workdir + ' \"' \n","          + input_geotiff + '\" \"' \n","          + output_pred_file + '\"')\n"],"metadata":{"id":"JeBAs3dIDeZn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ตอนนี้ ท่านสามารถค้นหาผล testing ภาพถ่ายดาวเทียมได้ที่โฟลเดอร์ *test_results* ท่านสามารถเรียกดูผลลัพธ์ได้โดยดาวน์โหลด GeoTiffs (*.tif) บน QGIS ด้านล่างนี้คือตัวอย่างการแสดงผลการทดสอบใน QGIS\n","\n","*ใส่ตัวอย่างแสดงผลการทดสอบ*"],"metadata":{"id":"kqQ88Kj-GoSG"}},{"cell_type":"markdown","source":["### 2-3-2 การประเมินความแม่นยำโดย by confusion matrix\n","\n","เราคำนวณ confusion matrix ระหว่าง prediction result และชั้นการใช้ที่ดิน confusion matrix และชั้นการใช้ที่ดิน cross-tabulate ของทั้งสองเลเยอร์ ที่นี่เราฝึกฝนในเซลล์ด้านล่าง"],"metadata":{"id":"8SzjDNuoHcwr"}},{"cell_type":"code","source":["ground_truth_file = \"vec/Rubber plantation, Year 2018.gpkg\"\n","prediction_result_file = \"test_results/2-3-1 Prachuap A, Year 2018.tif.pred.tif\"\n","\n","import geopandas as gpd\n","import rasterio\n","from rasterio import features\n","from sklearn import metrics\n","\n","ground_truth = gpd.read_file(ground_truth_file)\n","geom = [shapes for shapes in ground_truth.geometry]\n","prediction_result = rasterio.open(prediction_result_file)\n","\n","ground_truth_rasterized = features.rasterize(geom,\n","                                out_shape = prediction_result.shape,\n","                                fill = 0,\n","                                out = None,\n","                                transform = prediction_result.transform,\n","                                all_touched = False,\n","                                default_value = 1,\n","                                dtype = None)\n","\n","ground_truth_rasterized_1d_array = ground_truth_rasterized.ravel()\n","prediction_result_1d_array = prediction_result.read(1).ravel()\n","\n","confusion_matrix = metrics.confusion_matrix(ground_truth_rasterized_1d_array, prediction_result_1d_array)\n","iou =  confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[0,1] + confusion_matrix[1,0])\n","print(\"IoU:       %.4f\" % iou)\n","\n","print(\"Accuracy:  %.4f\" % metrics.accuracy_score(ground_truth_rasterized_1d_array, prediction_result_1d_array))\n","print(\"Precision: %.4f\" % metrics.precision_score(ground_truth_rasterized_1d_array, prediction_result_1d_array))\n","print(\"Recall:    %.4f\" % metrics.recall_score(ground_truth_rasterized_1d_array, prediction_result_1d_array))\n"],"metadata":{"id":"7KimDESjqJZH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.4. ใช้ trained model กับภาพถ่ายดาวเทียมล่าสุด\n","\n","วัตถุประสงค์ของส่วนนี้คือเพื่ออธิบายว่าสามารถนำ pre-trained model กลับมาใช้ใหม่เพื่อสร้างการคาดการณ์ได้อย่างไร โดยเฉพาะอย่างยิ่งสำหรับการสร้างแผนที่นาข้าวใหม่ ประเด็นการเรียนรู้ที่สำคัญ ได้แก่ การทำความเข้าใจถึงประโยชน์ของการใช้ pre-trained models สำหรับการจัดทำแผนที่การใช้ที่ดิน ตลอดจนข้อดีและข้อจำกัดของการผลิตแผนที่ตามโมเดล"],"metadata":{"id":"etL3aNVkpZ--"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y620JDggJ1Il"},"outputs":[],"source":["## Specify the model trained in Section 2-2 ##\n","model_file = '2-2_checkpoint.hdf5'\n","##############################################\n","\n","import os\n","os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n","import segmentation_models as sm\n","\n","# Set a model architecture and backbone same to the model for loading.\n","model = sm.FPN('efficientnetb2', classes=1, activation='sigmoid')\n","# Set a path to the trained model file for testing.\n","model.load_weights(model_file)"]},{"cell_type":"markdown","source":["จากนั้นนำภาพถ่ายดาวเทียมมาประมวลผลเพื่อใช้เป็นโมเดลเหมือนข้อ 2-3"],"metadata":{"id":"RvzIKjgAKzvr"}},{"cell_type":"code","source":["input_geotiff = '2-4_img/Prachuap A, Year 2020.tif'\n","output_pred_file = \"test_results/2-4 Prachuap A, Year 2020.tif.pred.tif\"\n","test_patch_size = 256\n","\n","import os, tempfile, shutil, glob\n","import numpy as np\n","from PIL import Image\n","os.environ['SM_FRAMEWORK'] = \"tf.keras\"\n","import segmentation_models as sm\n","\n","workdir = tempfile.mkdtemp()\n","os.system('bash semantic_segmentation_training/split_images_for_testing.sh \"'  # https://github.com/GLODAL/semantic_segmentation_training/blob/main/split_images_for_testing.sh \n","          + input_geotiff + '\" ' \n","          + workdir + ' ' \n","          + str(test_patch_size) )\n","\n","os.makedirs(workdir + '/pat_prd/', exist_ok=True)\n","for f in glob.glob(workdir + '/pat_img/*.tif'):\n","  raw = Image.open(f)\n","  out_patch_size = raw.size[0]\n","  raw = np.array(raw)\n","  raw = np.array(raw)/255.\n","  raw = raw[:,:,0:3]\n","\n","  pred = model.predict(np.expand_dims(raw, 0))\n","  pred[pred >= 0.5] = 1\n","  pred[pred < 0.5] = 0\n","  pred = pred.astype(np.uint8).reshape([out_patch_size,out_patch_size])\n","  im = Image.fromarray(pred)\n","  im.save(workdir + '/pat_prd/' + os.path.basename(f) + '.pred.tif')\n","\n","os.system('bash semantic_segmentation_training/merge_annotation_for_testing.sh ' # https://github.com/GLODAL/semantic_segmentation_training/blob/main/merge_annotation_for_testing.sh\n","          + workdir + ' \"' \n","          + input_geotiff + '\" \"' \n","          + output_pred_file + '\"')\n"],"metadata":{"id":"rA4KV0qsLGy0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ตอนนี้ ท่านสามารถค้นหาtesting result ภาพถ่ายดาวเทียมได้ที่โฟลเดอร์ *test_results* ท่านสามารถเรียกดูผลลัพธ์ได้โดยดาวน์โหลด GeoTiffs (*.tif) บน QGIS ด้านล่างนี้คือตัวอย่างการแสดงผลการทดสอบใน QGIS\n","เนื่องจากเรามีชั้นการใช้ประโยชน์ที่ดินสำหรับปี 2021 ท่านจึงสามารถประเมินความแม่นยำของการคาดการณ์สำหรับปี 2021 โดยเปรียบเทียบกับชั้นการใช้ประโยชน์ที่ดินผ่านกระบวนการเช่นเดียวกับ 2-3-2 เราปล่อยให้เป็นแบบฝึกหัดเสริมในเซสชันนี้"],"metadata":{"id":"ESL7ECnHLVFU"}},{"cell_type":"markdown","source":["## 2.5. ตรวจหาตำแหน่งที่มีการเปลี่ยนแปลงครั้งใหญ่ที่สุด\n","ส่วนย่อยนี้มีจุดมุ่งหมายเพื่อให้เข้าใจแนวคิดของการตรวจจับการเปลี่ยนแปลงและวิธีนำเทคนิค deep learning มาใช้เพื่อตรวจจับการเปลี่ยนแปลงการใช้ที่ดิน\n","\n","ประเด็นการเรียนรู้ที่สำคัญของส่วนย่อยนี้ ได้แก่ :\n","*   ระบุการเปลี่ยนแปลงที่จะตรวจจับได้\n","*   ทำความเข้าใจเกี่ยวกับคุณภาพของ outputs และวิธีจัดการเพื่อตรวจจับสถานที่ที่มีการเปลี่ยนแปลงการใช้ที่ดินอย่างมีนัยสำคัญ\n","\n","ส่วนย่อยนี้จะดำเนินการโดยใช้ QGIS\n","\n"],"metadata":{"id":"MLuG_J0Ey2Fm"}},{"cell_type":"markdown","source":["การวิเคราะห์จะต้องเป็นโพลีกอนเพื่อตรวจจับการใช้ประโยชน์ที่ดินที่มีการเปลี่ยนแปลงมากที่สุด เราแปลงผลการทำนายเป็นโพลีกอนในเซลล์ด้านล่าง"],"metadata":{"id":"rK6qmw3PRuOD"}},{"cell_type":"code","source":["baseline_land_use_file = \"vec/Rubber plantation, Year 2018.gpkg\"\n","predicted_land_use_raster_file = \"test_results/2-4 Prachuap A, Year 2020.tif.pred.tif\"\n","predicted_land_use_vector_file = \"test_results/2-4 Prachuap A, Year 2020.geojson\"\n","import os\n","os.system('gdal_polygonize.py \"'\n","        + predicted_land_use_raster_file\n","        + '\" -f GeoJSON \"'\n","        + predicted_land_use_vector_file + '\"'\n","        )\n","\n","\n","\n","\n"],"metadata":{"id":"35TKhtfQRmcH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import geopandas\n","\n","gpd_predicted_land_use = geopandas.read_file(predicted_land_use_vector_file)\n","gpd_baseline_land_use  = geopandas.read_file(baseline_land_use_file).clip(gpd_predicted_land_use.total_bounds)\n","gpd_predicted_land_use = gpd_predicted_land_use[gpd_predicted_land_use['DN']==1]\n"],"metadata":{"id":"vyS3A3MhjWiN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","gpd_baseline_land_use.overlay(gpd_predicted_land_use, how='union')\n","\n","\n","\n"],"metadata":{"id":"2_N2sCvZlrkr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gdal_polygonize.py \"test_results/2-4 Prachuap A, Year 2020.tif.pred.tif\" -f GeoJSON test.geojson"],"metadata":{"id":"Mt8URFj0gZSA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ส่วนที่ 3: การถ่ายทอดการเรียนรู้ของโมเดลที่ผ่านการเทรน \n","\n","ส่วนนี้ประกอบด้วยแนวทางปฏิบัติของการถ่ายโอนการเรียนรู้ ซึ่งเป็นเทคนิคสำหรับการใช้โมเดลที่เคยผ่านการเทรนซ้ำกับโดเมนอื่นๆ เพื่อประหยัดเวลาและทรัพยากรแทนการฝึกโมเดลตั้งแต่เริ่มต้น ส่วนนี้มีโครงสร้างโดยมีวิธีปฏิบัติดังต่อไปนี้\n","\n","- 3-1 ทดสอบโมเดลที่ได้รับการเทรนไปยังภูมิภาคอื่นๆ\n","- 3-2 ปรับแต่งโมเดลที่ได้รับการเทรนไปยังภูมิภาคอื่นๆ\n","- 3-3 เทรนโมเดลโดยรวมกับชุดข้อมูลการเทรน\n","\n","หลังจากจบส่วนนี้ท่านจะมีความเข้าใจในข้อจำกัดของโมเดลที่ผ่านการเทรนและขั้นตอนที่จำเป็นสำหรับการใช้โมเดลที่ผ่านการเทรนในขอบเขตทางภูมิศาสตร์ที่กว้างขึ้น\n","\n","ในส่วนนี้ เรากำหนดเงื่อนไขของ \"ภูมิภาค A\" และ \"ภูมิภาค B\" ดังนี้:\n","\n","- ภูมิภาค A: ภูมิภาคของชุดข้อมูลที่ใช้ในโมเดลเทรนนิ่ง\n","- ภูมิภาค B: ภูมิภาคของชุดข้อมูลสำหรับการทดสอบโมเดลที่ได้รับการเทรนสำหรับภูมิภาค A"],"metadata":{"id":"4uSOyZLuk2Il"}},{"cell_type":"markdown","source":["## 3-1  ทดสอบโมเดลที่ผ่านการเทรนไปยังภูมิภาคอื่นๆ\n","\n","ในส่วนย่อยนี้ ท่านมีการทดลองเล็กๆ ในการทดสอบโมเดลที่ผ่านการเทรนกับชุดข้อมูลอื่นที่ไม่ใช่ชุดข้อมูลการเทรน ท่านจะเห็นประสิทธิภาพที่ไม่ดีของโมเดล เนื่องจากโมเดลถูกจำกัดไปยังภูมิภาคของชุดข้อมูลการเทรน"],"metadata":{"id":"nSHs4XAZntnt"}},{"cell_type":"markdown","source":["ขั้นแรก ให้โหลดโมเดลที่ผ่านการเทรนซึ่งสร้างในส่วนที่ 2-2"],"metadata":{"id":"eH8_Qetiotz9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"C1YLxfBaiIES"},"outputs":[],"source":["## Specify the model trained in Section 2-2 ##\n","model_file = 'demo_model.hdf5'\n","##############################################\n","\n","import os\n","os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n","\n","import segmentation_models as sm\n","import tensorflow_addons as tfa\n","sm.set_framework('tf.keras')\n","\n","# Set a model architecture and backbone same to the model for loading.\n","model = sm.FPN('efficientnetb2', classes=1, activation='sigmoid')\n","# Set a path to the trained model file for testing.\n","model.load_weights(model_file)"]},{"cell_type":"markdown","source":["ต่อไป ใช้โมเดลที่โหลดมากับชุดข้อมูลรูปภาพจากภูมิภาคอื่น"],"metadata":{"id":"5SkMQyrWpAHM"}},{"cell_type":"code","source":["input_geotiff = '3_img/Prachuap B, Year 2018.tif'\n","output_pred_file = \"test_results/3-1 Prachuap B, Year 2018.tif.pred.tif\"\n","test_patch_size = 256\n","\n","import os, tempfile, shutil, glob\n","import numpy as np\n","from PIL import Image\n","os.environ['SM_FRAMEWORK'] = \"tf.keras\"\n","import segmentation_models as sm\n","\n","workdir = tempfile.mkdtemp()\n","os.system('bash semantic_segmentation_training/split_images_for_testing.sh \"' \n","          + input_geotiff + '\" ' \n","          + workdir + ' ' \n","          + str(test_patch_size) )\n","\n","os.makedirs(workdir + '/pat_prd/', exist_ok=True)\n","for f in glob.glob(workdir + '/pat_img/*.tif'):\n","  raw = Image.open(f)\n","  out_patch_size = raw.size[0]\n","  raw = np.array(raw)\n","  raw = np.array(raw)/255.\n","  raw = raw[:,:,0:3]\n","\n","  pred = model.predict(np.expand_dims(raw, 0))\n","  pred[pred >= 0.5] = 1\n","  pred[pred < 0.5] = 0\n","  pred = pred.astype(np.uint8).reshape([out_patch_size,out_patch_size])\n","  im = Image.fromarray(pred)\n","  im.save(workdir + '/pat_prd/' + os.path.basename(f) + '.pred.tif')\n","\n","os.system('bash semantic_segmentation_training/merge_annotation_for_testing.sh ' \n","          + workdir + ' \"' \n","          + input_geotiff + '\" \"' \n","          + output_pred_file + '\"')\n"],"metadata":{"id":"Caog7QKhpbRw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["จากนั้นคำนวณ perfomance index เช่น ความแม่นยำ(accuracy) และ IoU เพื่อเปรียบเทียบกับผลการทดสอบในส่วนที่ 2-3"],"metadata":{"id":"-fPdKOb-pejT"}},{"cell_type":"code","source":["ground_truth_file = \"vec/Rubber plantation, Year 2018.gpkg\"\n","prediction_result_file = \"test_results/3-1 Prachuap B, Year 2018.tif.pred.tif\"\n","\n","import geopandas as gpd\n","import rasterio\n","from rasterio import features\n","from sklearn import metrics\n","\n","ground_truth = gpd.read_file(ground_truth_file)\n","geom = [shapes for shapes in ground_truth.geometry]\n","prediction_result = rasterio.open(prediction_result_file)\n","\n","ground_truth_rasterized = features.rasterize(geom,\n","                                out_shape = prediction_result.shape,\n","                                fill = 0,\n","                                out = None,\n","                                transform = prediction_result.transform,\n","                                all_touched = False,\n","                                default_value = 1,\n","                                dtype = None)\n","\n","ground_truth_rasterized_1d_array = ground_truth_rasterized.ravel()\n","prediction_result_1d_array = prediction_result.read(1).ravel()\n","\n","confusion_matrix = metrics.confusion_matrix(ground_truth_rasterized_1d_array, prediction_result_1d_array)\n","iou =  confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[0,1] + confusion_matrix[1,0])\n","print(\"IoU:       %.4f\" % iou)\n","\n","print(\"Accuracy:  %.4f\" % metrics.accuracy_score(ground_truth_rasterized_1d_array, prediction_result_1d_array))\n","print(\"Precision: %.4f\" % metrics.precision_score(ground_truth_rasterized_1d_array, prediction_result_1d_array))\n","print(\"Recall:    %.4f\" % metrics.recall_score(ground_truth_rasterized_1d_array, prediction_result_1d_array))\n"],"metadata":{"id":"udSoM7TyiIET"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ท่านสามารถเรียกดูผลลัพธ์การแบ่งกลุ่ม(segmentation results) ได้โดยการโหลดเอาต์พุต GeoTiff บน QGIS ดาวน์โหลด GeoTiff ภายใต้ .... ด้านล่างนี้คือตัวอย่างที่แสดงเอาต์พุตบน QGIS\n","\n","ภาพจาก QGIS\n","\n","ที่นี่ คุณอาจะเข้าใจว่าโมเดลจำเป็นต้องมีกระบวนการเพิ่มเติมเพื่อใช้สำหรับภูมิภาคอื่นๆ"],"metadata":{"id":"YqTsYYGAqKiV"}},{"cell_type":"markdown","source":["## 3-2  ปรับแต่งโมเดลที่ผ่านการเทรนไปยังภูมิภาคอื่นๆ\n","\n","ในส่วนย่อยนี้ ท่านจะได้ฝึกฝนการปรับแต่งโมเดลที่ประสิทธิภาพยังไม่ดีใน 3-1 กระบวนการปรับแต่งโมเดลที่ได้รับการเทรนโดยใช้ชุดข้อมูลการเทรนอื่นจากภูมิภาค B เพื่อให้โมเดลสามารถจดจำรูปแบบในภูมิภาค B ได้"],"metadata":{"id":"bJBt4nyjsC8S"}},{"cell_type":"markdown","source":["ขั้นแรก ให้โหลดโมเดลที่ผ่านการเทรนซึ่งสร้างในส่วนที่ 2-2 ในครั้งนี้ โมเดลถูกโหลดด้วยเอ็นโค้ดเดอร์(encoders)ที่หยุดทำงาน ซึ่งช่วยรักษา weights ในโมเดลที่ฝึกไว้ล่วงหน้า"],"metadata":{"id":"feDbJBx5vxaz"}},{"cell_type":"code","source":["## Specify the model trained in Section 2-2 ##\n","model_file = 'demo_model.hdf5'\n","##############################################\n","\n","import os\n","os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n","\n","import segmentation_models as sm\n","import tensorflow_addons as tfa\n","sm.set_framework('tf.keras')\n","\n","# Set a model architecture and backbone same to the model for loading.\n","model = sm.FPN('efficientnetb2', classes=1, activation='sigmoid')\n","# Set a path to the trained model file for testing.\n","model.load_weights(model_file)"],"metadata":{"id":"n9HN5swtv24w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ต่อไป สร้าง training patch images จากภูมิภาค B"],"metadata":{"id":"iV2erC4Uv3hp"}},{"cell_type":"code","source":["# Configure parameters for patch images.\n","n_patch = 50\n","patch_size = 128\n","\n","in_vec_dir = 'vec/'\n","in_img_dir = '3_img/'\n","out_ann_dir = 'ann/'\n","out_patch_dir = '3-2_patch'\n","\n","import os, glob, random\n","import tensorflow as tf\n","import segmentation_models as sm\n","\n","os.system('bash semantic_segmentation_training/rasterize_polygon_layers.sh \"'\n","          + in_vec_dir + '\" \"'\n","          + in_img_dir + '\" \"'\n","          + out_ann_dir +'\"'\n","          )\n","\n","in_ann_dir = out_ann_dir\n","os.system('bash semantic_segmentation_training/gen_training_patches.sh \"' \n","         + in_img_dir + '\" \"' \n","          + in_ann_dir + '\" ' \n","          + str(patch_size) + ' '\n","          + str(n_patch) + ' '\n","          + out_patch_dir\n","          )\n"],"metadata":{"id":"5I66RcCViIET"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["จากนั้นเทรนโมเดลที่โหลดโดยใช้ patch images ที่สร้างขึ้น ลองประมวลผล epoch ด้วยจำนวนน้อย เช่น 5"],"metadata":{"id":"fyWlIfMzwFP9"}},{"cell_type":"code","source":["# Configure parameters for model training,\n","n_epoch = 5\n","batch_size =16\n","validation_split=0.2\n","checkpoint_file = '3-2_checkpoint.hdf5'\n","\n","\n","import numpy as np\n","from PIL import Image\n","\n","# generating batches of images and their corresponding masks\n","def image_generator(files, batch_size = batch_size, sz = (patch_size, patch_size)):\n","  while True: \n","    \n","    #extract a random batch \n","    batch = np.random.choice(files, size = batch_size)    \n","    \n","    #variables for collecting batches of inputs and outputs \n","    batch_x = []\n","    batch_y = []\n","    \n","    for f in batch:\n","        #get the masks. Note that masks are png files \n","        try:\n","          mask = Image.open(f.replace(\"patch_img\",\"patch_ann\"))\n","        except:\n","          continue\n","        mask = np.array(mask.resize(sz))\n","        batch_y.append(mask)\n","\n","        #preprocess the raw images \n","        raw = Image.open(f)\n","        raw = raw.resize(sz)\n","        raw = np.array(raw)\n","\n","        #check the number of channels because some of the images are RGBA or GRAY\n","        if len(raw.shape) == 2:\n","          raw = np.stack((raw,)*3, axis=-1)\n","        else:\n","          raw = raw[:,:,0:3]\n","        \n","        batch_x.append(raw)\n","\n","    #preprocess a batch of images and masks \n","    batch_x = np.array(batch_x)/255.\n","    batch_y = np.array(batch_y)\n","    batch_y = np.expand_dims(batch_y,axis = 3)\n","\n","    yield (batch_x, batch_y.astype(np.float32))\n","\n","# list of all files in the directory specified by patch_img\n","all_files = glob.glob(out_patch_dir + '/patch_img/*.tif')\n","\n","# randomly shuffles the order of the files in all_files\n","random.shuffle(all_files) \n","\n","#split into training and testing\n","split = int((1-validation_split) * len(all_files))\n","train_files = all_files[0:split]\n","test_files  = all_files[split:]\n","\n","train_generator = image_generator(train_files, batch_size = batch_size) # creates a generator for the training set \n","test_generator  = image_generator(test_files, batch_size = batch_size) # creates a generator for the testing set \n","\n","# retrieves the next batch of images and masks from the training set generator\n","x, y= next(train_generator)\n","\n","\n","import tensorflow_addons as tfa\n","model.compile(\n","    optimizer = tfa.optimizers.RectifiedAdam(), # tf.keras.optimizers.Adam(),\n","    loss = sm.losses.DiceLoss(),\n","    metrics = ['accuracy',sm.metrics.iou_score] \n",")\n","\n","import datetime\n","\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_file,\n","    save_weights_only=True,\n","    save_best_only=True)\n","\n","\n","# performing model training \n","import math\n","train_step = math.ceil(len(glob.glob(out_patch_dir+'/patch_img/*.tif')) * (1 - validation_split) / batch_size)\n","test_step = math.ceil(len(glob.glob(out_patch_dir+'/patch_img/*.tif')) * validation_split / batch_size)\n","\n","model_history = model.fit(train_generator, \n","                          epochs = n_epoch, \n","                          steps_per_epoch = train_step,\n","                          validation_data = test_generator, \n","                          validation_steps = test_step,\n","                          callbacks = model_checkpoint_callback, \n","                          verbose = 1)\n"],"metadata":{"id":"B-vdHAO2SRQa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ทดสอบโมเดลที่ผ่านการเทรนโดยปรับแต่งกับชุดข้อมูลใหม่"],"metadata":{"id":"lkP8Ce5QSBpR"}},{"cell_type":"code","source":["## Specify the model trained in Section 3-2 ##\n","model_file = '3-2_checkpoint.hdf5'\n","##############################################\n","\n","input_geotiff = '3_img/Prachuap B, Year 2018.tif'\n","output_pred_file = 'test_results/3-2 Prachuap B, Year 2018.tif.pred.tif'\n","test_patch_size = 256\n","\n","import os\n","os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n","import segmentation_models as sm\n","\n","# Set a model architecture and backbone same to the model for loading.\n","model = sm.FPN('efficientnetb2', classes=1, activation='sigmoid')\n","# Set a path to the trained model file for testing.\n","model.load_weights(model_file)\n","\n","import os, tempfile, shutil, glob\n","import numpy as np\n","from PIL import Image\n","\n","workdir = tempfile.mkdtemp()\n","os.system('bash semantic_segmentation_training/split_images_for_testing.sh \"' \n","          + input_geotiff + '\" ' \n","          + workdir + ' ' \n","          + str(test_patch_size) )\n","\n","os.makedirs(workdir + '/pat_prd/', exist_ok=True)\n","for f in glob.glob(workdir + '/pat_img/*.tif'):\n","  raw = Image.open(f)\n","  out_patch_size = raw.size[0]\n","  raw = np.array(raw)\n","  raw = np.array(raw)/255.\n","  raw = raw[:,:,0:3]\n","\n","  pred = model.predict(np.expand_dims(raw, 0))\n","  pred[pred >= 0.5] = 1\n","  pred[pred < 0.5] = 0\n","  pred = pred.astype(np.uint8).reshape([out_patch_size,out_patch_size])\n","  im = Image.fromarray(pred)\n","  im.save(workdir + '/pat_prd/' + os.path.basename(f) + '.pred.tif')\n","\n","os.system('bash semantic_segmentation_training/merge_annotation_for_testing.sh ' \n","          + workdir + ' \"' \n","          + input_geotiff + '\" \"' \n","          + output_pred_file + '\"')\n"],"metadata":{"id":"SnrUYCNxSK69"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["สุดท้าย คำนวณ perfomance indicators เช่น ความแม่นยำ(accuracy)และ IoU เพื่อเปรียบเทียบกับผลการทดสอบใน 3-1"],"metadata":{"id":"HS39MBk3wMxF"}},{"cell_type":"code","source":["ground_truth_file = \"vec/Rubber plantation, Year 2018.gpkg\"\n","prediction_result_file = \"test_results/3-2 Prachuap B, Year 2018.tif.pred.tif\"\n","\n","import geopandas as gpd\n","import rasterio\n","from rasterio import features\n","from sklearn import metrics\n","\n","ground_truth = gpd.read_file(ground_truth_file)\n","geom = [shapes for shapes in ground_truth.geometry]\n","prediction_result = rasterio.open(prediction_result_file)\n","\n","ground_truth_rasterized = features.rasterize(geom,\n","                                out_shape = prediction_result.shape,\n","                                fill = 0,\n","                                out = None,\n","                                transform = prediction_result.transform,\n","                                all_touched = False,\n","                                default_value = 1,\n","                                dtype = None)\n","\n","ground_truth_rasterized_1d_array = ground_truth_rasterized.ravel()\n","prediction_result_1d_array = prediction_result.read(1).ravel()\n","\n","confusion_matrix = metrics.confusion_matrix(ground_truth_rasterized_1d_array, prediction_result_1d_array)\n","iou =  confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[0,1] + confusion_matrix[1,0])\n","print(\"IoU:       %.4f\" % iou)\n","\n","print(\"Accuracy:  %.4f\" % metrics.accuracy_score(ground_truth_rasterized_1d_array, prediction_result_1d_array))\n","print(\"Precision: %.4f\" % metrics.precision_score(ground_truth_rasterized_1d_array, prediction_result_1d_array))\n","print(\"Recall:    %.4f\" % metrics.recall_score(ground_truth_rasterized_1d_array, prediction_result_1d_array))\n"],"metadata":{"id":"Fho2dQdHwtw0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ท่านสามารถเรียกดูผลลัพธ์การแบ่งกลุ่ม(segmentation results) ได้โดยการโหลดเอาต์พุต GeoTiff บน QGIS ดาวน์โหลด GeoTiff ภายใต้ .... ด้านล่างนี้คือตัวอย่างที่แสดงเอาต์พุตบน QGIS\n","\n","ภาพจาก QGIS\n","\n","ท่านอาจเห็นประสิทธิภาพของโมเดลที่ดีกว่าการทดสอบใน 3-1 นี่เป็นผลลัพธ์ของกระบวนการปรับแต่ง ซึ่งปรับปรุงประสิทธิภาพของโมเดลแม้ว่าจะมี epochs จำนวนน้อยก็ตาม"],"metadata":{"id":"Fnt4pRUYxgVG"}},{"cell_type":"markdown","source":["## 3-3 เทรนโมเดลโดยรวมกับชุดข้อมูลการเทรน\n","ในส่วนย่อยนี้ ท่านมีการทดลองการเทรนโมเดลโดยใช้ชุดข้อมูลจากภูมิภาค A และภูมิภาค B โมเดลที่ผ่านการเทรนคาดหวังว่าจะทำงานได้ดีสำหรับทั้งสองภูมิภาค เนื่องจากโมเดลได้เรียนรู้รูปแบบของภูมิภาค อย่างไรก็ตาม ท่านจะเห็นว่ามันต้องใช้การวนซ้ำและ epochs เยอะกว่าจะได้ประสิทธิภาพที่ดีกว่าการปรับแต่งแบบใน 3-2"],"metadata":{"id":"q1XMVvoox4XF"}},{"cell_type":"markdown","source":["สร้าง training patch images จากทั้งสองภูมิภาค ติดตั้งสอง GeoTiffs สำหรับการสร้าง patch images"],"metadata":{"id":"kbLAzLb6C9jr"}},{"cell_type":"code","source":["!rm -rf 3-3_patch"],"metadata":{"id":"wF3Teo_bf7NT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Configure parameters for patch images.\n","n_patch = 25\n","patch_size = 128\n","\n","in_vec_dir = 'vec/'\n","in_img_dir = '3-3_train_img/'\n","out_ann_dir = 'ann/'\n","out_patch_dir = '3-3_patch'\n","\n","import os, glob, random\n","import tensorflow as tf\n","import segmentation_models as sm\n","\n","os.system('bash semantic_segmentation_training/rasterize_polygon_layers.sh \"'\n","          + in_vec_dir + '\" \"'\n","          + in_img_dir + '\" \"'\n","          + out_ann_dir +'\"'\n","          )\n","\n","in_ann_dir = out_ann_dir\n","os.system('bash semantic_segmentation_training/gen_training_patches.sh \"' \n","         + in_img_dir + '\" \"' \n","          + in_ann_dir + '\" ' \n","          + str(patch_size) + ' '\n","          + str(n_patch) + ' '\n","          + out_patch_dir\n","          )\n"],"metadata":{"id":"ukRNyfHnzoY2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["จากนั้นเทรนโมเดลโดยใช้ patch images ที่สร้างขึ้นตั้งแต่เริ่มต้น เราเทรนไว้เพียง 5 epochs เพื่อเปรียบเทียบกับโมเดลใน 3-2"],"metadata":{"id":"ElnDoI0wDVdF"}},{"cell_type":"code","source":["# Configure parameters for model training,\n","n_epoch = 5\n","batch_size =16\n","validation_split=0.2\n","checkpoint_file = '3-3_checkpoint_epoch_5.hdf5'\n","\n","\n","import numpy as np\n","from PIL import Image\n","\n","# generating batches of images and their corresponding masks\n","def image_generator(files, batch_size = batch_size, sz = (patch_size, patch_size)):\n","  while True: \n","    \n","    #extract a random batch \n","    batch = np.random.choice(files, size = batch_size)    \n","    \n","    #variables for collecting batches of inputs and outputs \n","    batch_x = []\n","    batch_y = []\n","    \n","    for f in batch:\n","        #get the masks. Note that masks are png files \n","        try:\n","          mask = Image.open(f.replace(\"patch_img\",\"patch_ann\"))\n","        except:\n","          continue\n","        mask = np.array(mask.resize(sz))\n","        batch_y.append(mask)\n","\n","        #preprocess the raw images \n","        raw = Image.open(f)\n","        raw = raw.resize(sz)\n","        raw = np.array(raw)\n","\n","        #check the number of channels because some of the images are RGBA or GRAY\n","        if len(raw.shape) == 2:\n","          raw = np.stack((raw,)*3, axis=-1)\n","        else:\n","          raw = raw[:,:,0:3]\n","        \n","        batch_x.append(raw)\n","\n","    #preprocess a batch of images and masks \n","    batch_x = np.array(batch_x)/255.\n","    batch_y = np.array(batch_y)\n","    batch_y = np.expand_dims(batch_y,axis = 3)\n","\n","    yield (batch_x, batch_y.astype(np.float32))\n","\n","# list of all files in the directory specified by patch_img\n","all_files = glob.glob(out_patch_dir + '/patch_img/*.tif')\n","\n","# randomly shuffles the order of the files in all_files\n","random.shuffle(all_files) \n","\n","#split into training and testing\n","split = int((1-validation_split) * len(all_files))\n","train_files = all_files[0:split]\n","test_files  = all_files[split:]\n","\n","train_generator = image_generator(train_files, batch_size = batch_size) # creates a generator for the training set \n","test_generator  = image_generator(test_files, batch_size = batch_size) # creates a generator for the testing set \n","\n","# retrieves the next batch of images and masks from the training set generator\n","x, y= next(train_generator)\n","\n","\n","import tensorflow_addons as tfa\n","model.compile(\n","    optimizer = tfa.optimizers.RectifiedAdam(), # tf.keras.optimizers.Adam(),\n","    loss = sm.losses.DiceLoss(),\n","    metrics = ['accuracy',sm.metrics.iou_score] \n",")\n","\n","import datetime\n","\n","model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_file,\n","    save_weights_only=True,\n","    save_best_only=True)\n","\n","\n","# performing model training \n","import math\n","train_step = math.ceil(len(glob.glob(out_patch_dir+'/patch_img/*.tif')) * (1 - validation_split) / batch_size)\n","test_step = math.ceil(len(glob.glob(out_patch_dir+'/patch_img/*.tif')) * validation_split / batch_size)\n","\n","model_history = model.fit(train_generator, \n","                          epochs = n_epoch, \n","                          steps_per_epoch = train_step,\n","                          validation_data = test_generator, \n","                          validation_steps = test_step,\n","                          callbacks = model_checkpoint_callback, \n","                          verbose = 1)\n"],"metadata":{"id":"dFgEfymqERXU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ลองทดสอบโมเดลที่ผ่านการเทรนไปยังภูมิภาค B เนื่องจากโมเดลไม่ได้รับการเทรนใน epochs ที่เพียงพอ ประสิทธิภาพจึงน้อยกว่าโมเดลใน 3-1"],"metadata":{"id":"je489SQ0ESwx"}},{"cell_type":"code","source":["## Specify the model trained in Section 3-2 ##\n","model_file = '3-3_checkpoint_epoch_5.hdf5'\n","##############################################\n","\n","input_geotiff = '3_img/Prachuap B, Year 2018.tif'\n","output_pred_file = 'test_results/3-3 epoch_5 Prachuap B, Year 2018.tif.pred.tif'\n","test_patch_size = 256\n","\n","import os\n","os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n","\n","import segmentation_models as sm\n","import tensorflow_addons as tfa\n","sm.set_framework('tf.keras')\n","\n","# Set a model architecture and backbone same to the model for loading.\n","model = sm.FPN('efficientnetb2', classes=1, activation='sigmoid')\n","# Set a path to the trained model file for testing.\n","model.load_weights(model_file)\n","\n","import os, tempfile, shutil, glob\n","import numpy as np\n","from PIL import Image\n","os.environ['SM_FRAMEWORK'] = \"tf.keras\"\n","import segmentation_models as sm\n","sm.set_framework('tf.keras')\n","\n","workdir = tempfile.mkdtemp()\n","os.system('bash semantic_segmentation_training/split_images_for_testing.sh \"' \n","          + input_geotiff + '\" ' \n","          + workdir + ' ' \n","          + str(test_patch_size) )\n","\n","os.makedirs(workdir + '/pat_prd/', exist_ok=True)\n","for f in glob.glob(workdir + '/pat_img/*.tif'):\n","  raw = Image.open(f)\n","  out_patch_size = raw.size[0]\n","  raw = np.array(raw)\n","  raw = np.array(raw)/255.\n","  raw = raw[:,:,0:3]\n","\n","  pred = model.predict(np.expand_dims(raw, 0))\n","  pred[pred >= 0.5] = 1\n","  pred[pred < 0.5] = 0\n","  pred = pred.astype(np.uint8).reshape([out_patch_size,out_patch_size])\n","  im = Image.fromarray(pred)\n","  im.save(workdir + '/pat_prd/' + os.path.basename(f) + '.pred.tif')\n","\n","os.system('bash semantic_segmentation_training/merge_annotation_for_testing.sh ' \n","          + workdir + ' \"' \n","          + input_geotiff + '\" \"' \n","          + output_pred_file + '\"')\n"],"metadata":{"id":"pdk2ElgSEfkV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ground_truth_file = \"vec/Rubber plantation, Year 2018.gpkg\"\n","prediction_result_file = \"test_results/3-3 epoch_5 Prachuap B, Year 2018.tif.pred.tif\"\n","\n","import geopandas as gpd\n","import rasterio\n","from rasterio import features\n","from sklearn import metrics\n","\n","ground_truth = gpd.read_file(ground_truth_file)\n","geom = [shapes for shapes in ground_truth.geometry]\n","prediction_result = rasterio.open(prediction_result_file)\n","\n","ground_truth_rasterized = features.rasterize(geom,\n","                                out_shape = prediction_result.shape,\n","                                fill = 0,\n","                                out = None,\n","                                transform = prediction_result.transform,\n","                                all_touched = False,\n","                                default_value = 1,\n","                                dtype = None)\n","\n","ground_truth_rasterized_1d_array = ground_truth_rasterized.ravel()\n","prediction_result_1d_array = prediction_result.read(1).ravel()\n","\n","confusion_matrix = metrics.confusion_matrix(ground_truth_rasterized_1d_array, prediction_result_1d_array)\n","iou =  confusion_matrix[1,1] / (confusion_matrix[1,1] + confusion_matrix[0,1] + confusion_matrix[1,0])\n","print(\"IoU:       %.4f\" % iou)\n","\n","print(\"Accuracy:  %.4f\" % metrics.accuracy_score(ground_truth_rasterized_1d_array, prediction_result_1d_array))\n","print(\"Precision: %.4f\" % metrics.precision_score(ground_truth_rasterized_1d_array, prediction_result_1d_array))\n","print(\"Recall:    %.4f\" % metrics.recall_score(ground_truth_rasterized_1d_array, prediction_result_1d_array))\n"],"metadata":{"id":"2JFmkATNfION"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ต่อไป มาเทรนโมเดลสำหรับ ***** epochs กัน โมเดลจะมีประสิทธิภาพดีกว่าโมเดลที่ได้รับการฝึกฝนด้วยแค่ 5 epochs "],"metadata":{"id":"Ji4WMpKmEhna"}},{"cell_type":"code","source":["##"],"metadata":{"id":"aUN8mZIoEl7q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["ทดสอบโมเดลที่ผ่านการเทรนไปยังภูมิภาค B ท่านอาจเห็นประสิทธิภาพที่ดีกว่าโมเดลที่ผ่านการฝึกอบรมเพียงแค่ 5 epochs "],"metadata":{"id":"xWD9Gl8iEnP6"}},{"cell_type":"code","source":["##"],"metadata":{"id":"LWcsk4ORGBAG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["โมเดลที่ผ่านการเทรนปฏิบัติการได้น้อยลงใน epochs จำนวนเท่ากัน ซึ่งโมเดลที่ปรับแต่งแล้วนั้นมีประสิทธิภาพที่ใช้งานได้จริง การเปรียบเทียบเผยให้เห็นว่าการปรับแต่งประหยัดเวลาสำหรับการเทรนโมเดลที่เหมาะสมสำหรับภูมิภาค B\n","\n","หากท่านสามารถเผื่อเวลาไว้มากสำหรับการเทรนโมเดล การเทรนตั้งแต่เริ่มต้นคือตัวเลือกในการพัฒนาโมเดล อย่างไรก็ตาม อาจทำให้เกิดค่าใช้จ่ายสูงเมื่อท่านขยายการดำเนินงานไปยังภูมิภาคกว้างๆ"],"metadata":{"id":"XL4dY8p1GCBJ"}}]}